{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c76431-4ab8-4690-b59b-89aeb7b8e553",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@author: Kealan Hennessy\n",
    "@author: Maziar Raissi\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import time\n",
    "from Burgers_custom import PhysicsInformedNN\n",
    "\n",
    "np.random.seed(1234)\n",
    "tf.random.set_seed(1234)\n",
    "tf.keras.backend.set_floatx(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e1886c-9b9f-4ac1-a129-2d92639d1999",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fpath = './burgers_shock.mat'\n",
    "lambda_1_true, lambda_2_true = 1.0, (0.01/np.pi)\n",
    "\n",
    "def get_all(fpath):\n",
    "    data = scipy.io.loadmat(fpath)\n",
    "    x, t, u_exact = data['x'], data['t'], np.real(data['usol']).T\n",
    "    xx, tt = np.meshgrid(x, t)\n",
    "    X_all = np.hstack((xx.flatten()[:,None], tt.flatten()[:,None]))\n",
    "    y_all = u_exact.flatten()[:,None]\n",
    "    lb = X_all.min(0) # lower domain bound\n",
    "    ub = X_all.max(0) # upper domain bound\n",
    "    return X_all, y_all, lb, ub\n",
    "\n",
    "def pick_cpoints(X_all, y_all, N_u):\n",
    "    idx = np.random.choice(np.shape(X_all)[0], N_u, replace=False)\n",
    "    X, y = X_all[idx,:], y_all[idx,:]\n",
    "    return X, y\n",
    "\n",
    "def add_noise(u_train, noise):\n",
    "    noisy_u_train = u_train + noise*np.std(u_train)*np.random.randn(u_train.shape[0], u_train.shape[1])\n",
    "    return noisy_u_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c466d52-876e-4446-9667-d08a4ee05189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_loop(N_u, noise, num_layers, num_units, activation):\n",
    "    X_all, y_all, lb, ub = get_all(data_fpath)\n",
    "    X_train, y_train = pick_cpoints(X_all, y_all, N_u)\n",
    "    if noise:\n",
    "        y_train = add_noise(y_train, noise)\n",
    "\n",
    "    model = PhysicsInformedNN(X_train, y_train, num_layers, num_units, lb, ub, activation)\n",
    "    model.train(nIter_Adam=5000, use_LBFGSB=True)\n",
    "\n",
    "    u_pred, f_pred = model.predict(X_all)\n",
    "    lambda_1_val = model.lambda_1\n",
    "    lambda_2_val = np.exp(model.lambda_2)\n",
    "    \n",
    "    error_u = np.divide(np.linalg.norm(y_all - u_pred, 2), np.linalg.norm(y_all, 2))\n",
    "    pct_error_l1 = np.abs(lambda_1_val - lambda_1_true) * 100\n",
    "    pct_error_l2 = (np.divide(np.abs(lambda_2_val - lambda_2_true), (0.01/np.pi))) * 100\n",
    "\n",
    "    print('Error u: %e' % (error_u))\n",
    "    print('Error l1: %.5f%%' % (pct_error_l1[0]))\n",
    "    print('Error l2: %.5f%%' % (pct_error_l2[0]))\n",
    "\n",
    "    return pct_error_l1[0], pct_error_l2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe03414c-5380-4dde-bb7d-19815a1d0528",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_u = [500, 1000, 1500, 2000]\n",
    "noise = [0.0, 0.01, 0.05, 0.1]\n",
    "num_layers = [2, 4, 6, 8]\n",
    "num_units = [10, 20, 40]\n",
    "error_lambda_1_table_1 = np.zeros((len(N_u), len(noise)))\n",
    "error_lambda_2_table_1 = np.zeros((len(N_u), len(noise)))\n",
    "error_lambda_1_table_2 = np.zeros((len(num_layers), len(num_units)))\n",
    "error_lambda_2_table_2 = np.zeros((len(num_layers), len(num_units)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90bee99-5835-4bc0-83d4-861a1990e45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print('*'*50, 'Beginning systemic discovery loop', '*'*50)\n",
    "\n",
    "for i in range(len(N_u)):\n",
    "    for j in range(len(noise)):\n",
    "        print('*'*30, 'Beginning discovery: ', 'Num. of collocation points:', N_u[i], ', Noise:', int(noise[j]*100), '%', '*'*30)\n",
    "        error_lambda_1_table_1[i,j], error_lambda_2_table_1[i,j] = main_loop(N_u[i], noise[j], num_layers[-1], num_units[-1], 'relu')\n",
    "        print('*'*50, 'Discovery complete', '*'*50)\n",
    "\n",
    "for i in range(len(num_layers)):\n",
    "    for j in range(len(num_units)):\n",
    "        print('*'*30, 'Beginning discovery: ', 'Num. of surrogate layers:', num_layers[i], ', Num. of surrogate units:', num_units[j], '*'*30)\n",
    "        error_lambda_1_table_2[i,j], error_lambda_2_table_2[i,j] = main_loop(N_u[-1], noise[0], num_layers[i], num_units[j], 'relu')\n",
    "        print('*'*50, 'Discovery complete', '*'*50)\n",
    "\n",
    "print('*'*50, 'Systemic discovery loop complete', '*'*50)\n",
    "end_time = time.time()\n",
    "print('*'*50, \"Time to completion:\", (np.abs(start_time - end_time)), '*'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e0b812-e1af-4db8-b2dd-447db06b4faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('./tables/error_lambda_1_table_1.csv', error_lambda_1_table_1, delimiter=' & ', fmt='$%2.3f$', newline=' \\\\\\\\\\n')\n",
    "np.savetxt('./tables/error_lambda_2_table_1.csv', error_lambda_2_table_1, delimiter=' & ', fmt='$%2.3f$', newline=' \\\\\\\\\\n')\n",
    "np.savetxt('./tables/error_lambda_1_table_2.csv', error_lambda_1_table_2, delimiter=' & ', fmt='$%2.3f$', newline=' \\\\\\\\\\n')\n",
    "np.savetxt('./tables/error_lambda_2_table_2.csv', error_lambda_2_table_2, delimiter=' & ', fmt='$%2.3f$', newline=' \\\\\\\\\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
